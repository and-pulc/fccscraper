{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfminer as pm\n",
    "import math\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta  \n",
    "import warnings\n",
    "import os\n",
    "import subprocess\n",
    "import ocrmypdf\n",
    "import sys\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p1t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-257291b6814c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p1t' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(p1t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in all table regions\n",
    "def convertPDFToTable(filepath):\n",
    "    table_cols = [['35,52,97,136,210,256,308,360,390,422,456,469,490,513,550',\n",
    "                  '35,52,97,136,210,256,308,360,390,422,456,469,490,513,550',\n",
    "                  '35,52,97,136,210,256,308,360,390,422,456,469,490,513,550',\n",
    "                  '35,52,97,136,210,256,308,360,390,422,456,469,490,513,550',\n",
    "                  '35,52,97,136,210,256,308,360,390,422,456,469,490,513,550'], \n",
    "                  ['44,']]\n",
    "    curColSetting = 0\n",
    "    p1t = camelot.read_pdf(filepath, \n",
    "                         strip_text='.\\n', flavor='stream', pages=\"1-end\",\n",
    "                         #table_regions=['0,700,600,0'],\n",
    "                          columns=table_cols[curColSetting], \n",
    "                           edge_tol=100)\n",
    "    pdf = []\n",
    "    for t in p1t:\n",
    "        pdf.append(t.df)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processOrder(tables, committee, station):\n",
    "    progColre=False\n",
    "    ## Create the ad day/times df.\n",
    "    ads = pd.DataFrame(columns=['Start Date', 'End Date', 'Weekdays', 'Spots/Week', 'Rate', 'Rating', 'index'])\n",
    "    ## Create the shows DF.\n",
    "    shows = pd.DataFrame(columns=['index', 'Title'])\n",
    "    secondlines = pd.DataFrame(columns=['index', 'Title'])\n",
    "    ## Add second line of inventory code\n",
    "    def mergeSLines(row):\n",
    "        if row.name+1 < len(shows):\n",
    "            nextRow = shows.iloc[row.name+1]\n",
    "            sline = secondlines[(row['index'] < secondlines['index']) & (nextRow['index']>secondlines['index'])]\n",
    "        else: \n",
    "            sline = secondlines[(row['index'] < secondlines['index'])]\n",
    "        if(len(sline)>0):\n",
    "            if(row['Title'] != sline['Title'].values[0]):\n",
    "                row['Title'] = row['Title'] + sline['Title'].values[0]\n",
    "        else:\n",
    "            print('Second line merge error at ad: ', row.name)\n",
    "        return row\n",
    "    \n",
    "    def getSecondLines(df):\n",
    "        returnVal = pd.DataFrame(columns=['index', 'Title'])\n",
    "        #Get show lines\n",
    "        progs = df[(df['Amount'] != '') & (df['Ch'] != '')]\n",
    "        progs = progs.iloc[1:] # Remove residual header row.\n",
    "        slines = list(map(lambda x: x+1, progs.index.values))\n",
    "        if not (slines[-1] < len(df)): ## In case the second show line is on the next page.\n",
    "            print('Program second line on next page.')\n",
    "            slines = slines[:-1]\n",
    "        slines = df.iloc[slines]\n",
    "        slines = slines.replace('', np.nan, regex=True)\n",
    "        slines = slines[slines.isnull().sum(axis=1) >= len(slines.columns)-2]\n",
    "        for index, row in slines.iterrows():\n",
    "            nulls = pd.isnull(row)\n",
    "            nulls = nulls[nulls!=True]\n",
    "            returnVal = returnVal.append({'index': index, 'Title': row[nulls.index[0]]}, ignore_index=True)\n",
    "        returnVal['index'] = returnVal['index'] + lastIndex\n",
    "        return returnVal\n",
    "    def addNameIfDiff(p):\n",
    "        for val in nlines.iloc[p.name]:\n",
    "            if(p[progColName] != val) & (val.isna() != True):\n",
    "                p[progColName] = p[progColName] + val\n",
    "        return p\n",
    "    ## Merge program names into ad days.\n",
    "    def mergeProgs(i):\n",
    "        p = shows[shows['index']<=i]\n",
    "        if len(p)>0:\n",
    "            p = p.iloc[len(p)-1]\n",
    "            return p['Title']\n",
    "        else:\n",
    "            return \"\"\n",
    "    ## Create individual records for each ad spot.  \n",
    "    ## Filter out day times rows.                \n",
    "    def getDays(row):\n",
    "        #print(row)\n",
    "        nextRow = 1\n",
    "        dayCols = ['Start Date', 'End Date', 'Weekdays', 'Spots/Week', 'Rate']\n",
    "        for val in row:\n",
    "            try:\n",
    "                ind = dayCols.index(val)\n",
    "                dayCols.remove(val)\n",
    "            except:\n",
    "                pass\n",
    "        if(len(dayCols) == 0):\n",
    "            if(len(headers)==0):\n",
    "                headers.append(buys.loc[(row.name), :])\n",
    "            while (nextRow!=-1):\n",
    "                dayColFound=False\n",
    "                if((row.name+nextRow) != len(buys)):\n",
    "                    potRow = buys.loc[(row.name+nextRow), :]\n",
    "                    valCount=0\n",
    "                    for k in potRow:\n",
    "                        if 'Week:' in k:\n",
    "                            if k.split(':')[1]:\n",
    "                                fixVal = k.split(':')[1]\n",
    "                                potRow[valCount+1] = fixVal\n",
    "                            nextRow=nextRow+1\n",
    "                            dayColFound=True\n",
    "                        valCount = valCount+1\n",
    "                    if dayColFound != True:\n",
    "                        nextRow=-1\n",
    "                    else:\n",
    "                        headers.append(potRow)\n",
    "                else:\n",
    "                    nextRow=-1\n",
    "    def findHeaderRow(row):\n",
    "        headers =  { 'Description': ['Start Date End Date Description'],\n",
    "                     'Inventory Code': ['Amount', 'Start', 'Inventory Code', 'Rate', 'Spots'] }\n",
    "        for htype in headers:\n",
    "            for val in row:\n",
    "                try:\n",
    "                    ind = headers[htype].index(val)\n",
    "                    headers[htype].remove(val)\n",
    "                except:\n",
    "                    pass\n",
    "        if(len(headers['Description']) == 0):\n",
    "            return ['Description', row.name]\n",
    "        else:\n",
    "            if(len(headers['Inventory Code']) == 0):\n",
    "                return ['Inventory Code', row.name]\n",
    "    def fixHeaders(row):\n",
    "        fixCol = -1\n",
    "        for k in row.keys():\n",
    "            if row[k] == 'Start Date End Date Description':\n",
    "                fixCol = k\n",
    "        if fixCol != -1:\n",
    "            if(fixCol == 3):\n",
    "                row[(fixCol-2)] = 'Ch'\n",
    "                row[(fixCol-1)] = 'Start'\n",
    "                row[(fixCol)] = 'End'\n",
    "                row[(fixCol+1)] = 'Description'\n",
    "                row[(fixCol+2)] = 'Desc2'\n",
    "            if(fixCol == 2):\n",
    "                row[(fixCol)] = 'Start'\n",
    "                row[(fixCol+1)] = 'End'\n",
    "                row[(fixCol+2)] = 'Description'\n",
    "        return row\n",
    "    ## Filter out program names.\n",
    "    lastIndex = 0\n",
    "    for t in tables:\n",
    "        progColName=False\n",
    "        headerConfig = t.apply(findHeaderRow, axis=1).dropna()\n",
    "        if len(headerConfig)>0:\n",
    "            progColName = headerConfig.values[0][0]\n",
    "        if progColName != False:\n",
    "            ## Figure out inventory code lines\n",
    "            progs = t.copy()\n",
    "            if progColName == 'Description':\n",
    "                progs = progs.apply(fixHeaders, axis=1)\n",
    "                progs.columns = progs.iloc[headerConfig.values[0][1]]\n",
    "            else:\n",
    "                progs.columns = progs.iloc[headerConfig.values[0][1]]\n",
    "            progs = progs.iloc[headerConfig.values[0][1]:]\n",
    "            progs = progs.reset_index(drop=True)\n",
    "            ## Some WideOrbit reports have two line program names, this gets the second lines.\n",
    "            if progColName == 'InventoryCode':\n",
    "                slines = getSecondLines(progs)\n",
    "                secondlines = secondlines.append(slines)\n",
    "            ## Get the program name rows\n",
    "            progs = progs[(progs['Amount'] != '') & (progs['Ch'] != '')] \n",
    "            progs = progs.iloc[1:] # Remove residual header row.\n",
    "            if progColName == 'Description':\n",
    "                progs['Description'] = progs['Description'] + progs['Desc2']\n",
    "            progs = progs.reset_index()\n",
    "            ## Make index run continously accross pages.\n",
    "            progs['index'] = progs['index'] + lastIndex\n",
    "            lastIndex = t.iloc[len(t)-1].name+lastIndex+1\n",
    "            ## Remove duplicate columns and append to the master list.\n",
    "            progs = progs.loc[:,~progs.columns.duplicated()]\n",
    "            progs = progs.rename(columns={progColName:'Title'})\n",
    "            progs = progs[['index', 'Title']]\n",
    "            shows = shows.append(progs, sort=False)\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"False\")\n",
    "    ## Merge in the second lines to complete the program names.\n",
    "    if progColName == 'InventoryCode':\n",
    "        shows = shows.reset_index(drop=True)\n",
    "        shows = shows.apply(mergeSLines, axis=1)\n",
    "    ## Seperate out the days and times each ad will be on.\n",
    "    lastIndex = 0\n",
    "    for t in tables:\n",
    "        progColName=False\n",
    "        headerConfig = t.apply(findHeaderRow, axis=1).dropna()\n",
    "        if len(headerConfig)>0:\n",
    "            progColName = headerConfig.values[0][0]\n",
    "        if progColName != False:\n",
    "            headers = []\n",
    "            buys = t.copy()\n",
    "            buys = buys.dropna(how='all')\n",
    "            buys = buys.iloc[headerConfig.values[0][1]:]\n",
    "            buys = buys.reset_index(drop=True)\n",
    "            buys.apply(getDays, axis=1)\n",
    "            days = pd.DataFrame(headers)\n",
    "            days = days.replace('', np.nan)\n",
    "            days = days.dropna(axis='columns')\n",
    "            days.columns = days.iloc[0]\n",
    "            days = days[1:]\n",
    "            days = days.reset_index()\n",
    "            days['index'] = days['index'] + lastIndex\n",
    "            lastIndex = t.iloc[len(t)-1].name+lastIndex+1\n",
    "            ads = ads.append(days, sort=False)\n",
    "    ## Add in Program Names\n",
    "    ads['Program'] = ads['index'].apply(mergeProgs)\n",
    "    ads = ads.drop(columns=['index'], axis=1)\n",
    "    '''\n",
    "    print('SHOWS')\n",
    "    print(shows)\n",
    "    print('ADS')\n",
    "    print(ads[['index', 'Weekdays']])\n",
    "    print(len(adtimes))\n",
    "    '''\n",
    "    ## Expand each ad buy listing to individual spots\n",
    "    return ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCyclePDFs(cyc, basepath, test):\n",
    "    fulladlist = pd.DataFrame(columns=['Date', 'Rate', 'Committee', 'Station', 'Program'])\n",
    "    pdfResults = pd.DataFrame(columns=['Path', 'Result'])\n",
    "    kw = pd.read_csv('/media/andrew/F08C9B848C9B444E/analysis/tv/fccscraper/keys/filetypekeywords.csv') # keywords\n",
    "    malads = pd.DataFrame(columns=['Start Date', 'End Date', 'Weekdays',\n",
    "                                   'Spots/Week', 'Rate', 'Rating'])\n",
    "    def digToNextLevel(folderpath, name, files):\n",
    "        nonlocal pdfResults\n",
    "        nonlocal fulladlist\n",
    "        nonlocal kw\n",
    "        adtimes = [] # Final add array\n",
    "        stationads = pd.DataFrame(columns=['Start Date', 'End Date', 'Weekdays',\n",
    "                                           'Spots/Week', 'Rate', 'Rating'])\n",
    "        def expandDays(x):\n",
    "            adStartDate = datetime.strptime(x['Start Date'], '%m%d%y')\n",
    "            # Test if adbuy string is malformed.\n",
    "            for i, day in enumerate(x['Weekdays']):\n",
    "                if(day!='-'):\n",
    "                    if(day.isdigit()):\n",
    "                        for j in range(0, int(day)):\n",
    "                            adtimes.append({\n",
    "                                        'Date': (adStartDate + timedelta(days=i)),  \n",
    "                                        'Rate': x['Rate'],\n",
    "                                        'Committee': name,\n",
    "                                        'Station': station,\n",
    "                                        'Program': x['Program']\n",
    "                                       })\n",
    "                    else: ## CODE FOR MTW NOTATION\n",
    "                        adtimes.append({\n",
    "                                        'Date': (adStartDate + timedelta(days=i)),  \n",
    "                                        'Rate': x['Rate'],\n",
    "                                        'Committee': name,\n",
    "                                        'Station': station,\n",
    "                                        'Program': x['Program']\n",
    "                                       })\n",
    "        def classifyReportFormat(res, kw):\n",
    "            # Process the scraped text\n",
    "            res = res.split('\\n')\n",
    "            res = [x.replace(' ', '') for x in res ]\n",
    "            res = [x.replace(':', '') for x in res ]\n",
    "            res = [x.replace('.', '') for x in res ]\n",
    "            res = list(filter(lambda a: a != '', res))\n",
    "            res = res[0:200]\n",
    "            res = list(set(res))\n",
    "            # Generate keys from csv and score based on keys\n",
    "            keyAr = kw['pdftype'].values\n",
    "            ptypes = {}\n",
    "            for pt in keyAr:\n",
    "                ptkw = kw[kw['pdftype']==pt]\n",
    "                ptypes[pt] = len(ptkw[ptkw['keyword'].isin(res)])\n",
    "                ptypes[pt] = ptypes[pt]/len(ptkw)\n",
    "            maxVal = max(ptypes, key=ptypes.get)\n",
    "            if ptypes[maxVal] > .2:\n",
    "                return maxVal\n",
    "            else:\n",
    "                return False\n",
    "        def checkIntegrity(ad): # Checks ads are in valid format, performs common corrections due to OCR errors.\n",
    "            nonlocal stationads\n",
    "            malformed=False\n",
    "            # Date\n",
    "            for d in ['Start Date', 'End Date']:\n",
    "                ad[d] = ad[d].replace('/', '')\n",
    "                ad[d] = ad[d].replace('o', '0')\n",
    "                ad[d] = ad[d].replace('O', '0')\n",
    "                ad[d] = ad[d].replace('g', '9')\n",
    "                try:\n",
    "                    datetime.strptime(ad[d], '%m%d%y')\n",
    "                except:\n",
    "                    malformed=True            \n",
    "            # Rate\n",
    "            ad['Rate'] = ad['Rate'].split('$')[1]\n",
    "            ad['Rate'] = ad['Rate'].replace(',','')\n",
    "            try:\n",
    "                ad['Rate'] = ad['Rate'][:-2]\n",
    "                ad['Rate'] = float(ad['Rate'])\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                malformed=True\n",
    "            # Weekday\n",
    "            validDateChars=['M', 'T', 'W', 'h', 'F', 'S', 'a', 'u', 'H', 'A', 'U']\n",
    "            if len(ad['Weekdays']) != 7:\n",
    "                malformed=True\n",
    "            for char in ad['Weekdays']:\n",
    "                if char.isalpha():\n",
    "                    if char not in validDateChars:\n",
    "                        malformed=True\n",
    "            if malformed!=True:\n",
    "                stationads = stationads.append(ad, ignore_index=True, sort=False)\n",
    "            # Maybe malformed sheet else?\n",
    "        def processPDF(item, station, name):\n",
    "            nonlocal pdfResults\n",
    "            nonlocal fulladlist\n",
    "            nonlocal stationads\n",
    "            pdfProcessors = {'orders': processOrder, 'contracts': processOrder}\n",
    "            result = subprocess.run(['pdftotext', item, '-'], \n",
    "                                        stdout=subprocess.PIPE).stdout.decode()\n",
    "            if((len(result.split('\\n')[0:30])> 5)):\n",
    "                reportFormat = classifyReportFormat(result, kw)\n",
    "                if (reportFormat != False) & (reportFormat in pdfProcessors != False) :\n",
    "                    print(item)\n",
    "                    pdfTable = convertPDFToTable(item)\n",
    "                    try:\n",
    "                        ads = pdfProcessors[reportFormat](pdfTable, name, station)\n",
    "                        display(ads)\n",
    "                        if len(ads) > 0:\n",
    "                            ads.apply(checkIntegrity, axis=1)\n",
    "                            pdfResults = pdfResults.append({'Path': item, \n",
    "                                                            'Result': reportFormat+' Success'}, ignore_index=True, sort=False)\n",
    "                        else:\n",
    "                            pdfResults = pdfResults.append({'Path': item, \n",
    "                                            'Result': reportFormat+' Scraping Error'}, ignore_index=True)\n",
    "                    except:\n",
    "                        traceback.print_exc()\n",
    "                        pdfResults = pdfResults.append({'Path': item, \n",
    "                                            'Result': 'Scraping Error'}, ignore_index=True)\n",
    "                else:\n",
    "                    if reportFormat != False:\n",
    "                        pdfResults = pdfResults.append({'Path': item, \n",
    "                           'Result': reportFormat+'No parser written for file.'}, ignore_index=True)\n",
    "                    else:\n",
    "                        pdfResults = pdfResults.append({'Path': item, \n",
    "                           'Result': 'No parser written for file.'}, ignore_index=True)\n",
    "            else:\n",
    "                pdfResults = pdfResults.append({'Path': item, \n",
    "                   'Result': 'No parser written for file.'}, ignore_index=True)\n",
    "                ocrmypdf.ocr(item, item, deskew=True, rotate_pages=True)\n",
    "                processPDF(item, station, name)\n",
    "        for item in files:\n",
    "            processPDF((os.path.join(folderpath, item)), station, name)\n",
    "        if len(stationads)>0:\n",
    "            stationads = stationads.drop_duplicates() # subset array arg to ignore columns\n",
    "            stationads.apply(expandDays, axis=1)\n",
    "            fulladlist = fulladlist.append(pd.DataFrame(adtimes), ignore_index=True)\n",
    "    for dirName, subdirList, fileList in os.walk(basepath):\n",
    "        if test:\n",
    "            station = 'test'\n",
    "            digToNextLevel(dirName, cyc, fileList)\n",
    "        else:\n",
    "            for station in list(os.scandir(basepath)):\n",
    "                station = station.name\n",
    "                cyclepath = basepath+station+'/Political Files/'+cyc+'/'\n",
    "                digToNextLevel(cyclepath, cyc, fileList)\n",
    "        fulladlist['Cycle'] = cyc\n",
    "    return {'ads': fulladlist, 'pdfs': pdfResults, 'malformedads': malads}\n",
    "results = parseCyclePDFs('2018', '/media/andrew/F08C9B848C9B444E/analysis/tv/buys/PHOENIX (PRESCOTT)/KTAZ/Political Files/2018/Federal/US Senate/SINEMA/', test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Path, dtype: object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pacFromPath(p):\n",
    "    p = p.split('/')\n",
    "    station = p[(p.index('Political Files')-1)]\n",
    "    pac = p[len(p)-2]\n",
    "    if (station=='KNXV-TV') & (pac=='Majority Forward'):\n",
    "        print(p[len(p)-1])\n",
    "results['pdfs']['Path'].apply(pacFromPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13000'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1$13,00000'.split('$')[1][:-2].replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Committee</th>\n",
       "      <th>Station</th>\n",
       "      <th>Program</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, Rate, Committee, Station, Program]\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['ads'][results['ads']['Program'].str.contains('Prime')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62393"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocrmypdf.ocr('input.pdf', 'output.pdf', deskew=True, rotate_pages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "9\n",
      "6\n",
      "['Weekdays', 'StartDateEndDate', 'LengthWeek', 'Spots/', 'OriginalDate/Revision', 'PrintDate', 'Product1/2', 'Amount', 'AdvertiserCode', 'Start/End', 'Cash/Trade', 'Contract/Revision', 'Spots/Week']\n",
      "['BillingAddress', 'Deal#', 'Invoice#', 'AgencyRef', 'Order#', 'Property', 'Product', 'AdvertiserRef']\n",
      "['#Spots', 'OriginalDate/Rev', 'Estimate', 'Demographic', 'OrderSeparation', 'ORDER', 'PrimaryAE', 'BillPlan', 'Priority', 'UnitCode', 'Agency', 'GrossAmount', 'BillingCalendar', 'ProductCodes', 'Advertiser', 'FlightDates', 'Order%', 'Order/Rev', 'StartDate/EndDate', 'OrderType', 'AdvertiserExternalID', 'AccountExecutives', 'Month', 'BillingCycle', 'EOM/EOC', 'InventoryCode', 'BillingType', 'Start', 'Orders', 'AltOrder#', 'AgencyExternalID', 'RevenueCode2', 'AgencyCommission', 'Totals', 'SalesOffice', 'RevenueCode3', 'StartDate', 'SalesRegion', 'NewBusinessThru', 'BillingContact', 'NetAmount', 'AccountExecutive', 'RevenueCode1', '#SpotsGrossAmount', 'BuyingContact', 'ProductDesc', 'EndDate', 'Rating', 'General', 'Broadcast']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirName, subdirList, fileList in os.walk('/media/andrew/F08C9B848C9B444E/analysis/tv/buys/PHOENIX (PRESCOTT)/'):\n",
    "    print(dirName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
